import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats
from scipy.signal import savgol_filter
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import KFold
from sklearn.base import clone
from tqdm import tqdm

# ==========================================
# 1. ç‰©ç†æ¨¡å‹ï¼šç±³æ°æ–¹ç¨‹ (Michaelis-Menten)
# ==========================================

def f_phys(Y, D, theta):
    # dY/dt = - (Y * D) / (D + theta)
    denom = D + theta + 1e-8
    return - (Y * D) / denom

def grad_f_phys(Y, D, theta):
    # d(f)/d(theta) = (Y * D) / (D + theta)^2
    denom = (D + theta + 1e-8) ** 2
    return (Y * D) / denom

# ==========================================
# 2. æ•°æ®ç”Ÿæˆå™¨ (å«æ··æ‚ + å™ªéŸ³)
# ==========================================

def generate_ode_data(N=2000, true_theta=1.0, seed=None):
    if seed is not None:
        np.random.seed(seed)
    
    # Z: æ··æ‚å˜é‡ (å…ç–«åŠ›)
    Z = np.random.uniform(1.5, 3.5, N)
    
    # D: æ²»ç–— (å— Z å½±å“ï¼Œæ­£ç›¸å…³)
    D = 0.8 * Z + np.random.normal(0, 0.1, N)
    
    # Y: åˆå§‹çŠ¶æ€
    Y = np.random.uniform(5.0, 15.0, N)
    
    # çœŸå®ç‰©ç†å¯¼æ•°
    physics = f_phys(Y, D, true_theta)
    
    # æ··æ‚å¯¹å¯¼æ•°çš„å½±å“ g(Z)
    nuisance = -0.3 * Z**2 
    
    # æ€»å¯¼æ•° = ç‰©ç† + æ··æ‚ + è¿‡ç¨‹å™ªéŸ³
    # æ³¨æ„ï¼šè¿™é‡Œçš„å™ªéŸ³ U æ˜¯æ–¹å·®æ¥æºçš„ä¸€éƒ¨åˆ†
    U = np.random.normal(0, 0.2, N) 
    Y_dot_true = physics + nuisance + U
    
    # è§‚æµ‹å™ªéŸ³ (åŠ åœ¨ Y_dot ä¸Šæ¨¡æ‹Ÿæµ‹é‡è¯¯å·®æˆ–å¯¼æ•°ä¼°ç®—è¯¯å·®)
    Y_dot_obs = Y_dot_true + np.random.normal(0, 0.05, N)
    
    return Y, Y_dot_obs, D, Z

# ==========================================
# 3. å¸¦æ¨æ–­åŠŸèƒ½çš„ DML Solver
# ==========================================

def solve_ode_dml_inference(Y, Y_dot, D, Z, theta_init=0.5, n_splits=2, max_iter=7):
    theta = theta_init
    n = len(Y)
    
    # éšæœºæ£®æ—å‚æ•°
    rf = RandomForestRegressor(n_estimators=50, max_depth=5, min_samples_leaf=20, n_jobs=-1)
    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)
    
    # ç”¨äºå­˜å‚¨æœ€åä¸€æ¬¡è¿­ä»£çš„æ®‹å·®å’Œæ¢¯åº¦
    final_Y_tilde = None # å»åå¯¼æ•°æ®‹å·®
    final_G_tilde = None # å»åæ¢¯åº¦æ®‹å·®
    
    for k in range(max_iter):
        M = f_phys(Y, D, theta)
        J = grad_f_phys(Y, D, theta)
        R = Y_dot - M
        
        g_hat = np.zeros(n)
        h_hat = np.zeros(n)
        
        for train_idx, test_idx in kf.split(Y):
            Z_train, Z_test = Z[train_idx], Z[test_idx]
            R_train, J_train = R[train_idx], J[train_idx]
            
            m_bias = clone(rf).fit(Z_train.reshape(-1, 1), R_train)
            g_hat[test_idx] = m_bias.predict(Z_test.reshape(-1, 1))
            
            m_grad = clone(rf).fit(Z_train.reshape(-1, 1), J_train)
            h_hat[test_idx] = m_grad.predict(Z_test.reshape(-1, 1))
            
        # æ­£äº¤åŒ–
        Y_tilde = R - g_hat
        G_tilde = J - h_hat
        
        # ä¿å­˜ç”¨äºæ–¹å·®è®¡ç®—
        final_Y_tilde = Y_tilde
        final_G_tilde = G_tilde
        
        # Gauss-Newton æ›´æ–°
        num = np.dot(G_tilde, Y_tilde)
        den = np.dot(G_tilde, G_tilde)
        
        delta_theta = num / (den + 1e-8)
        theta = theta + 0.8 * delta_theta # Damping
        
        if theta < 0.01: theta = 0.01 # ç‰©ç†çº¦æŸ
        
        if np.abs(delta_theta) < 1e-5:
            break
            
    # === å…³é”®ï¼šæ–¹å·®æ¨å¯¼ (Sandwich Formula) ===
    # J_hat (Bread): æ¢¯åº¦çš„äºŒé˜¶çŸ©ï¼Œä»£è¡¨ä¿¡æ¯é‡
    J_hat = np.mean(final_G_tilde ** 2)
    
    # Sigma_hat (Meat): æ®‹å·®ä¸æ¢¯åº¦çš„ä¹˜ç§¯æ–¹å·®ï¼Œä»£è¡¨å™ªéŸ³
    Sigma_hat = np.mean((final_Y_tilde ** 2) * (final_G_tilde ** 2))
    
    # Asymptotic Variance Omega = J^-2 * Sigma
    Omega = Sigma_hat / (J_hat ** 2)
    
    # Standard Error = sqrt(Omega / N)
    se = np.sqrt(Omega / n)
    
    return theta, se

# ==========================================
# 4. è¿è¡Œè’™ç‰¹å¡æ´›å®éªŒ
# ==========================================

def run_inference_validation():
    TRUE_THETA = 1.0
    N_SAMPLES = 2000 # æ ·æœ¬é‡è¶³å¤Ÿå¤§ä»¥ä¿è¯æ¸è¿‘æ­£æ€æ€§
    N_SIMS = 200     # æ¨¡æ‹Ÿæ¬¡æ•° (å»ºè®® >100 ä»¥ç”»å‡ºå¹³æ»‘çš„ç›´æ–¹å›¾)
    
    t_stats = []
    estimates = []
    
    print(f"ğŸš€ æ­£åœ¨éªŒè¯ ODE æ¨¡å‹çš„ç»Ÿè®¡æ¨æ–­ (N={N_SAMPLES}, Sims={N_SIMS})...")
    
    for i in tqdm(range(N_SIMS)):
        # 1. ç”Ÿæˆæ•°æ®
        Y, Y_dot, D, Z = generate_ode_data(N=N_SAMPLES, true_theta=TRUE_THETA, seed=i)
        
        # 2. DML æ±‚è§£ (è·å– theta å’Œ se)
        theta_hat, se_hat = solve_ode_dml_inference(Y, Y_dot, D, Z, theta_init=0.5)
        
        # 3. è®¡ç®— t-statistic
        # t = (Estimate - Truth) / SE
        t = (theta_hat - TRUE_THETA) / se_hat
        
        t_stats.append(t)
        estimates.append(theta_hat)
        
    t_stats = np.array(t_stats)
    
    # ==========================================
    # 5. ç»˜å›¾ (ä½ è¦æ±‚çš„ä»£ç )
    # ==========================================
    plt.figure(figsize=(10, 6))
    
    # 1. ç»˜åˆ¶ t-statistics çš„ç›´æ–¹å›¾å’Œ KDE
    sns.histplot(t_stats, stat="density", bins=20, kde=True, 
                 color="skyblue", label=r"Empirical Distribution of $\frac{\hat{\theta} - \theta_0}{\hat{SE}}$",
                 edgecolor='white', alpha=0.6)
    
    # 2. ç»˜åˆ¶æ ‡å‡†æ­£æ€åˆ†å¸ƒ N(0, 1) çš„ç†è®ºæ›²çº¿
    x = np.linspace(-4, 4, 100)
    plt.plot(x, stats.norm.pdf(x, 0, 1), 'k--', linewidth=2.5, label=r"Standard Normal $\mathcal{N}(0, 1)$")
    
    plt.title(f"Validity of ODE DML Inference (N={N_SAMPLES})", fontsize=14)
    plt.xlabel("Standardized T-statistic", fontsize=12)
    plt.ylabel("Density", fontsize=12)
    plt.legend(fontsize=11)
    plt.grid(True, alpha=0.3)
    plt.xlim(-4, 4)
    
    plt.show()
    
    # --- æ‰“å°ç»Ÿè®¡éªŒè¯ ---
    # è®¡ç®—è¦†ç›–ç‡ (Coverage Rate)
    # ç†è®ºä¸Š 95% çš„ t-stat åº”è¯¥è½åœ¨ [-1.96, 1.96] ä¹‹é—´
    coverage = np.mean(np.abs(t_stats) < 1.96)
    
    print("\n" + "="*40)
    print(f"çœŸå®å‚æ•°: {TRUE_THETA}")
    print(f"ä¼°è®¡å‡å€¼: {np.mean(estimates):.4f}")
    print("-" * 40)
    print(f"95% CI è¦†ç›–ç‡ (ç›®æ ‡ 0.95): {coverage:.3f}")
    print(f"T-ç»Ÿè®¡é‡å‡å€¼ (ç›®æ ‡ 0.0):  {np.mean(t_stats):.3f}")
    print(f"T-ç»Ÿè®¡é‡æ–¹å·® (ç›®æ ‡ 1.0):  {np.var(t_stats):.3f}")
    print("="*40)

if __name__ == "__main__":
    run_inference_validation()
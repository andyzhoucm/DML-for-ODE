import torch
import torch.nn as nn
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import KFold
from scipy import stats
from tqdm import tqdm

# ==========================================
# 1. ç‰©ç†å±‚ï¼šå¯å¾®ç§¯åˆ†å™¨ (Differentiable RK4)
# ==========================================
class ODESolver(nn.Module):
    def __init__(self, func):
        super().__init__()
        self.func = func

    def forward(self, y0, D, dt, theta):
        """
        RK4 ç§¯åˆ†å™¨ï¼šè®¡ç®—ä» t åˆ° t+dt çš„çŠ¶æ€æ¼”åŒ–
        y_next = y_curr + Integral(f(y, D, theta))
        """
        # k1
        k1 = self.func(y0, D, theta)
        # k2
        k2 = self.func(y0 + 0.5 * dt * k1, D, theta)
        # k3
        k3 = self.func(y0 + 0.5 * dt * k2, D, theta)
        # k4
        k4 = self.func(y0 + dt * k3, D, theta)
        
        y_next = y0 + (dt / 6.0) * (k1 + 2 * k2 + 2 * k3 + k4)
        return y_next

def physics_model(y, D, theta):
    """
    ç‰©ç†æ–¹ç¨‹ï¼šMichaelis-Menten åŠ¨åŠ›å­¦
    dy/dt = - (Y * D) / (D + theta)
    """
    # é˜²æ­¢åˆ†æ¯ä¸º 0 åŠ ä¸€ä¸ªå° epsilon
    denom = D + theta + 1e-6
    return - (y * D) / denom

# ==========================================
# 2. æ•°æ®ç”Ÿæˆï¼šæ—¶å˜æ··æ‚ (Time-Varying Z)
# ==========================================
def generate_longitudinal_data(N=200, T=10, true_theta=1.0, dt=0.1, seed=None):
    if seed is not None:
        np.random.seed(seed)
        torch.manual_seed(seed)

    Y_list, D_list, Z_list = [], [], []

    # çœŸå®çš„æ»‹æ‰°å‡½æ•°
    def true_g(z):
        return 0.5 * z + np.sin(z)

    # ç‰©ç†æ–¹ç¨‹ (ç”¨äºæ•°æ®ç”Ÿæˆ)
    def true_physics_grad(y, d, theta):
        denom = d + theta + 1e-6
        return - (y * d) / denom

    for i in range(N):
        # 1. ç”Ÿæˆ Z(t)
        base_z = np.random.uniform(1.0, 3.0)
        time_trend = np.linspace(0, 1, T+1)
        Z_t = base_z + 0.5 * np.sin(2 * np.pi * time_trend) + np.random.normal(0, 0.1, T+1)
        
        # 2. ç”Ÿæˆ D(t)
        D_t = 0.5 * Z_t + np.random.normal(0.5, 0.1, T+1)
        
        # 3. é«˜ç²¾åº¦ç§¯åˆ†ç”Ÿæˆ Y(t)
        y_traj = [10.0 + np.random.normal(0, 0.5)] 
        
        for t in range(T):
            y_curr = y_traj[-1]
            z_curr = Z_t[t]
            d_curr = D_t[t]
            
            # --- æ ¸å¿ƒä¿®æ­£ï¼šä½¿ç”¨ RK4 ç”Ÿæˆæ•°æ® ---
            # æˆ‘ä»¬æŠŠ dynamics = physics + nuisance çœ‹ä½œä¸€ä¸ªæ•´ä½“
            
            def combined_dynamics(y_val, _d, _z):
                # _d å’Œ _z åœ¨ dt é—´éš”å†…è¿‘ä¼¼å¸¸æ•°ï¼Œæˆ–è€…ä½ å¯ä»¥æ’å€¼
                f_phys = true_physics_grad(y_val, _d, true_theta)
                f_nuis = true_g(_z)
                return f_phys + f_nuis

            # æ‰‹å†™ RK4 step
            k1 = combined_dynamics(y_curr, d_curr, z_curr)
            k2 = combined_dynamics(y_curr + 0.5*dt*k1, d_curr, z_curr)
            k3 = combined_dynamics(y_curr + 0.5*dt*k2, d_curr, z_curr)
            k4 = combined_dynamics(y_curr + dt*k3, d_curr, z_curr)
            
            dy = (dt / 6.0) * (k1 + 2*k2 + 2*k3 + k4)
            
            # åŠ ä¸Šè§‚æµ‹å™ªå£°
            y_next = y_curr + dy + np.random.normal(0, 0.05)
            
            y_traj.append(y_next)
            
        Y_list.append(y_traj)
        D_list.append(D_t)
        Z_list.append(Z_t)

    return (
        torch.tensor(Y_list, dtype=torch.float32),
        torch.tensor(D_list, dtype=torch.float32),
        torch.tensor(Z_list, dtype=torch.float32)
    )
# ==========================================
# 3. æ ¸å¿ƒç®—æ³•ï¼šå¸¦æ—©åœçš„ Integral DML
# ==========================================
class IntegralDML:
    def __init__(self, dt):
        self.dt = dt
        self.solver = ODESolver(physics_model)
        
    def fit_inference(self, Y, D, Z, n_splits=2, max_iter=20, tol=1e-4):
        """
        å‚æ•°:
        - max_iter: æœ€å¤§è¿­ä»£æ¬¡æ•° (æ¯”å¦‚ 20)
        - tol: æ”¶æ•›é˜ˆå€¼ (æ¯”å¦‚ 1e-4)
        """
        N, T_plus_1 = Y.shape
        T = T_plus_1 - 1
        
        # æ•°æ®æ‹æ‰ (Pooling)
        # é¢„æµ‹ Y(t+1) from Y(t)
        Y_curr_flat = Y[:, :-1].reshape(-1, 1)
        Y_next_flat = Y[:, 1:].reshape(-1, 1)
        D_curr_flat = D[:, :-1].reshape(-1, 1)
        Z_curr_flat = Z[:, :-1].reshape(-1, 1)
        
        # è®°å½•å—è¯•è€… ID ç”¨äºäº¤å‰éªŒè¯ (é˜²æ­¢ Time-leakage)
        subj_ids = np.repeat(np.arange(N), T)
        
        # åˆå§‹åŒ–å‚æ•°
        theta_est = torch.tensor([0.5], requires_grad=True)
        
        final_epsilon = None
        final_G_tilde = None
        
        # è¿­ä»£ä¼˜åŒ–å¾ªç¯
        for k in range(max_iter):
            # --- Step 1: ç‰©ç†é¢„æµ‹ & é›…å¯æ¯”è®¡ç®— ---
            if theta_est.grad is not None: theta_est.grad.zero_()
            
            # ç§¯åˆ†é¢„æµ‹: Y_pred = Phi(Y_t, D_t, theta)
            Y_pred = self.solver(Y_curr_flat, D_curr_flat, self.dt, theta_est)
            
            # è®¡ç®—é›…å¯æ¯” (æ¢¯åº¦): J = d(Phi)/d(theta)
            # è¿™é‡Œç”¨æœ‰é™å·®åˆ† (Finite Difference) ä¿è¯æ•°å€¼ç¨³å®šæ€§
            # ä½ ä¹Ÿå¯ä»¥ç”¨ torch.autograd.gradï¼Œä½†åœ¨æ ‡é‡å‚æ•°ä¸‹ FD å¾€å¾€æ›´ç¨³
            with torch.no_grad():
                delta_fd = 1e-4
                Y_pred_eps = self.solver(Y_curr_flat, D_curr_flat, self.dt, theta_est + delta_fd)
                J_raw = ((Y_pred_eps - Y_pred) / delta_fd).numpy().flatten()
            
            # åŸå§‹æ®‹å·® R (åŒ…å« nuisance * dt)
            R_raw = (Y_next_flat - Y_pred).detach().numpy().flatten()
            
            # --- Step 2: äº¤å‰æ‹Ÿåˆ (Cross-Fitting) ---
            g_hat_all = np.zeros_like(R_raw)
            h_hat_all = np.zeros_like(J_raw)
            
            kf = KFold(n_splits=n_splits, shuffle=True, random_state=42 + k) # æ¯æ¬¡è¿­ä»£éšæœºç§å­å˜ä¸€ä¸‹
            unique_subjs = np.unique(subj_ids)
            
            Z_numpy = Z_curr_flat.detach().numpy()
            
            for train_subj, val_subj in kf.split(unique_subjs):
                # å…³é”®ï¼šæŒ‰ Subject åˆ‡åˆ† Mask
                train_mask = np.isin(subj_ids, unique_subjs[train_subj])
                val_mask = np.isin(subj_ids, unique_subjs[val_subj])
                
                # ML 1: å­¦ä¹  Z -> Residual (ä¼°è®¡ç´¯ç§¯æ¼‚ç§»)
                # æ ‘çš„æ•°é‡ä¸ç”¨å¤ªå¤šï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆ
                ml_g = RandomForestRegressor(n_estimators=20, max_depth=5, min_samples_leaf=10, n_jobs=-1)
                ml_g.fit(Z_numpy[train_mask], R_raw[train_mask])
                g_hat_all[val_mask] = ml_g.predict(Z_numpy[val_mask])
                
                # ML 2: å­¦ä¹  Z -> Jacobian (ä¼°è®¡æ¢¯åº¦çš„æ¡ä»¶æœŸæœ›)
                ml_h = RandomForestRegressor(n_estimators=20, max_depth=5, min_samples_leaf=10, n_jobs=-1)
                ml_h.fit(Z_numpy[train_mask], J_raw[train_mask])
                h_hat_all[val_mask] = ml_h.predict(Z_numpy[val_mask])
            
            # --- Step 3: æ­£äº¤åŒ– (Orthogonalization) ---
            epsilon_tilde = R_raw - g_hat_all
            G_tilde = J_raw - h_hat_all
            
            # ä¿å­˜æœ€åä¸€æ­¥ç”¨äºç®—æ–¹å·®
            final_epsilon = epsilon_tilde
            final_G_tilde = G_tilde
            
            # --- Step 4: é«˜æ–¯-ç‰›é¡¿æ›´æ–° & ç»ˆæ­¢æ¡ä»¶ ---
            num = np.dot(G_tilde, epsilon_tilde)
            den = np.dot(G_tilde, G_tilde)
            
            # è®¡ç®—æ­¥é•¿
            delta_theta = num / (den + 1e-8)
            
            # === æ ¸å¿ƒä¿®æ”¹ï¼šæ—©åœæ£€æŸ¥ ===
            if np.abs(delta_theta) < tol:
                # è°ƒè¯•æ—¶å¯ä»¥å–æ¶ˆæ³¨é‡Šä¸‹é¢è¿™è¡ŒæŸ¥çœ‹åœ¨ç¬¬å‡ æ¬¡æ”¶æ•›
                # print(f"Converged at iter {k+1} with delta {delta_theta:.2e}")
                break
            
            # æ›´æ–°å‚æ•° (å¸¦é˜»å°¼ 0.8 é˜²æ­¢éœ‡è¡)
            new_val = theta_est.item() + 0.8 * delta_theta
            
            # ç‰©ç†çº¦æŸ: theta å¿…é¡»ä¸ºæ­£
            if new_val < 0.01: new_val = 0.01 
            
            theta_est = torch.tensor([new_val], requires_grad=True)
            
        # --- Step 5: æ¨æ–­ (Sandwich Formula) ---
        n_obs = len(final_epsilon)
        
        # J_hat (Bread): ä¿¡æ¯çŸ©é˜µ
        J_hat = np.mean(final_G_tilde ** 2)
        # Sigma_hat (Meat): å¾—åˆ†æ–¹å·®
        Sigma_hat = np.mean((final_epsilon ** 2) * (final_G_tilde ** 2))
        
        # æ¸è¿‘æ–¹å·® Omega
        Omega = Sigma_hat / (J_hat ** 2)
        se = np.sqrt(Omega / n_obs)
        
        return theta_est.item(), se

# ==========================================
# 4. è’™ç‰¹å¡æ´›å®éªŒè¿è¡Œ
# ==========================================
def run_simulation():
    TRUE_THETA = 1.0
    N_SIMS = 50         # æ¨¡æ‹Ÿæ¬¡æ•°
    N_SUBJECTS = 500    # æ ·æœ¬é‡
    T_STEPS = 10        # æ—¶é—´æ­¥é•¿
    DT = 0.1
    
    t_stats = []
    estimates = []
    
    print(f"ğŸš€ Running Integral DML with Early Stopping (Sims={N_SIMS})...")
    
    # å®ä¾‹åŒ– Solver
    dml = IntegralDML(dt=DT)
    
    for i in tqdm(range(N_SIMS)):
        # 1. ç”Ÿæˆæ•°æ®
        Y, D, Z = generate_longitudinal_data(N=N_SUBJECTS, T=T_STEPS, true_theta=TRUE_THETA, dt=DT, seed=i)
        
        # 2. æ‹Ÿåˆ (è®¾ç½® max_iter=20, tol=1e-4)
        theta_hat, se = dml.fit_inference(Y, D, Z, max_iter=20, tol=1e-4)
        
        print(f"Sim {i+1}/{N_SIMS}: Theta_hat={theta_hat:.4f}, SE={se:.4f}")

        # 3. ç»Ÿè®¡
        t = (theta_hat - TRUE_THETA) / se
        t_stats.append(t)
        estimates.append(theta_hat)
        
    t_stats = np.array(t_stats)
    
    # ==========================================
    # 5. ç»˜å›¾ä¸ç»“æœ
    # ==========================================
    plt.figure(figsize=(10, 6))
    
    # ç›´æ–¹å›¾ + KDE
    sns.histplot(t_stats, stat="density", bins=15, kde=True, 
                 color="skyblue", label=r"Empirical Distribution",
                 edgecolor='white', alpha=0.6)
    
    # æ ‡å‡†æ­£æ€åˆ†å¸ƒå‚è€ƒçº¿
    x = np.linspace(-4, 4, 100)
    plt.plot(x, stats.norm.pdf(x, 0, 1), 'k--', linewidth=2.5, label=r"Standard Normal $\mathcal{N}(0, 1)$")
    
    plt.title(f"Validity of Trajectory-Based Integral DML\n(Time-Varying Z, Early Stopping Enabled)", fontsize=14)
    plt.xlabel("Standardized T-statistic", fontsize=12)
    plt.ylabel("Density", fontsize=12)
    plt.legend(fontsize=11)
    plt.grid(True, alpha=0.3)
    plt.xlim(-4, 4)
    plt.show()

    # æ‰“å°ç»Ÿè®¡æŒ‡æ ‡
    coverage = np.mean(np.abs(t_stats) < 1.96)
    print("\n" + "="*40)
    print(f"True Theta: {TRUE_THETA}")
    print(f"Mean Estimate: {np.mean(estimates):.4f}")
    print("-" * 40)
    print(f"95% CI Coverage (Target 0.95): {coverage:.3f}")
    print(f"T-stat Mean (Target 0.0):      {np.mean(t_stats):.3f}")
    print(f"T-stat Var (Target 1.0):       {np.var(t_stats):.3f}")
    print("="*40)

if __name__ == "__main__":
    run_simulation()